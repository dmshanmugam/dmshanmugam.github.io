---
layout: default
--- 

Hi! I am a postdoc at Cornell, working with Emma Pierson. I completed my Ph.D. in the [Clinical and Applied Machine Learning](https://caml.csail.mit.edu/) group at MIT, where I was lucky to be advised by John Guttag. 
Before, I was at MIT for undergrad, where I majored in computer science with a concentration in South Asian studies. 

I work on machine learning for healthcare. My current research (often) falls into one or more of these categories:

<div style="background-color: #FFEDE0; padding: 0.75em; border-radius: 5px; border: #FFB085; margin-bottom: 0.5em; margin-top: 0em;">

<span style="color: #CB4B16;"> <b> Measuring human behavior in health datasets </b> </span> <br>
I believe that we can improve healthcare not just by training better predictive models, but also by building better *descriptive* models of how care is currently delivered.
<!-- To what extent are diseases under-diagnosed? 
How do financial incentives shape treatment decisions? These are questions that machine learning methods I develop, in combination with large health datasets, can answer.  -->

<ul>
> How can we measure the extent to which diseases are underdiagnosed in different patient subgroups? (NWH 2024) <br>
> How can we measure different patterns of health access? (under review) <br>
> How can we measure overtreatment? (in progress)
</ul>
</div>
<!-- Datasets used in machine learning for healthcare are shaped by a complex system of human interaction and financial incentives.
I like thinking about how we can develop machine learning methods to measure processes that shape observable data, in order to:
(1) better understand the ways in which observable data differs from the data we wish to observe and 
(2) reason about the behavior of machine learning models trained on this data. -->

<!-- I've thought about this in the context of *underdiagnosis* (of intimate partner violence (NWH 2024), and in ongoing work, genetic disease and cardiovascular outcomes),
*patterns of health access* (heart failure), and most recently, *overtreatment*. -->

<div style="background-color: #FFEDE0; padding: 0.75em; border-radius: 5px; border: #FFB085; margin-bottom: 0.5em; margin-top: 0em;">

 <span style="color: #CB4B16;"> <b> Updating and evaluating machine learning models </b> </span>  <br>

<!-- I am interested in issues that arise *after* training a model.  -->
While we are great at training models, there is room to improve the ways we update, evaluate, and select models. 
<!-- I work on new methods to update models using test-time augmentation, and new methods to evaluate machine learning models under domain-specific constraints. -->

<ul>
> How can we efficiently update models to be more accurate, robust, and calibrated? (ICCV 2021, under review) <br>
> How can we facilitate semantically-grounded, context-specific evaluation? (CHI 23) <br>
> How can we best evaluate classifiers in the absence of abundant labeled data? (under review)
</ul>
</div>

<div style="background-color: #FFEDE0; padding: 0.75em; border-radius: 5px; border: #FFB085; margin-bottom: 0.5em; margin-top: 0em;">
 <span style="color: #CB4B16;"> <b> Promoting health equity </b> </span>  <br>

Can we use AI to characterize and mitigate persistent health inequalities? I (and many of my co-authors) would <a url="https://arxiv.org/abs/2312.14804"> say yes</a>! 
I am especially committed to translating advances in machine learning to women's health. 

<!-- A number of open technical questions here motivate my current work,
including the scarcity of ground truth labels, and the role of predictive models in case management, particularly in the context of intimate partner violence and fertility. -->
<ul>
> How can we better measure the prevalence of intimate partner violence? (NWH 2024) <br>
> What features are important when predicting ovulation? (SR 2023) <br>
> What new opportunities in health equity do large language models enable? (NEJM AI 2025)
</ul>
</div>
Sometimes I describe my interests as "everything but model training". This is because 1. I am impatient and 2. I believe the road from messy to clean data and the road from trained model to deployment raise important unanswered questions.

<!-- If you are a student interested in collaborating on these topics, do reach out! -->

<!-- Future:
    EHR underdiagnosis, genetic underdiagnosis
    -->

<!-- Future:
    Birth Control Switches
    Birth prediction -->
<!-- There is increased interest in reducing, reusing, and recycling machine learning models, particularly in downstream applications.  -->

<!-- I also use the intersection of machine learning and healthcare as a "model organism" for the interface between practitioners and machine learning researchers.  -->
<!-- There are fundamental questions about updating and evaluating models that challenge us today. -->


<!-- (1) Building better *descriptive* models of how care is delivered today. To what extent are diseases under-diagnosed? 
How do financial incentives shape treatment decisions? These are questions that machine learning methods I develop, in combination with large health datasets, can answer.

(2) Building better methods to update and evaluate machine learning models for context-specific tasks. 

(3) Using machine learning to *promote* health equity.  -->


<!-- - How can we use test-time augmentation to update a model to be more accurate? (ICCV 2021,  ICML UDL Workshop 2023)
- How can we use test-time augmentation to be more robust to out-of-distribution examples? (ICML UDL Workshop 2022)
- How do we update a model to produce more reliable uncertainty estimates? (under review) -->

<!-- - What are the risks of using coarse race data to evaluate clinical risk scores? (MLHC 23)
- How can we facilitate semantically-grounded, context-specific evaluation (CHI 23)
- How can we best evaluate classifiers in the absence of abundant labeled data? (under review) -->